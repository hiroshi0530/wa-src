
## 第5章 顧客の退会を予測する10本ノック

この記事は[「Python実践データ分析100本ノック」](https://www.amazon.co.jp/dp/B07ZSGSN9S/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1)の演習を実際にやってみたという内容になっています。今まで自己流でやってきましたが、一度他の方々がどのような考え方やコーディングをしているのか勉強してみようと思ってやってみました。本書は実際の業務に活用する上でとても参考になる内容だと思っています。データ分析に関わる仕事をしたい方にお勧めしたいです。

本演習で利用しているデータは本サイトからは利用できません。ぜひとも「Python実践データ分析１００本ノック」を購入し、本に沿ってダウンロードして自分の手でコーディングしてみてください。（私は決して回し者ではないので安心してください笑）

前章（4章）では、クラスタリングと線形回帰を実行してみました。今回は決定木のようです。データ分析や予測において最初に使われるのがXGBoostやLightGBM、ランダムフォレストであり、それらの手法の基礎となっているのが決定木です。

### github
- jupyter notebook形式のファイルは[こちら](https://github.com/hiroshi0530/wa-src/blob/master/ml/data100/05/05_nb.ipynb)

### google colaboratory
- google colaboratory で実行する場合は[こちら](https://colab.research.google.com/github/hiroshi0530/wa-src/blob/master/ml/data100/05/05_nb.ipynb)

### 筆者の環境


```python
!sw_vers
```

    ProductName:	Mac OS X
    ProductVersion:	10.14.6
    BuildVersion:	18G6020



```python
!python -V
```

    Python 3.7.3


基本的なライブラリをインポートしそのバージョンを確認しておきます。


```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

import matplotlib
import matplotlib.pyplot as plt
import scipy
import numpy as np
import pandas as pd

print('matplotlib version :', matplotlib.__version__)
print('scipy version :', scipy.__version__)
print('numpy version :', np.__version__)
print('pandas version :', pd.__version__)
```

    matplotlib version : 3.0.3
    scipy version : 1.4.1
    numpy version : 1.16.2
    pandas version : 1.0.3


## 解答

### ノック : 41 データを読み込んで利用で他を整形しよう


```python
customer = pd.read_csv('customer_join.csv')
uselog_months = pd.read_csv('use_log_months.csv')
```


```python
customer.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>customer_id</th>
      <th>name</th>
      <th>class</th>
      <th>gender</th>
      <th>start_date</th>
      <th>end_date</th>
      <th>campaign_id</th>
      <th>is_deleted</th>
      <th>class_name</th>
      <th>price</th>
      <th>campaign_name</th>
      <th>mean</th>
      <th>median</th>
      <th>max</th>
      <th>min</th>
      <th>routine_flg</th>
      <th>calc_date</th>
      <th>membership_period</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>OA832399</td>
      <td>XXXX</td>
      <td>C01</td>
      <td>F</td>
      <td>2015-05-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>0</td>
      <td>オールタイム</td>
      <td>10500</td>
      <td>通常</td>
      <td>4.833333</td>
      <td>5.0</td>
      <td>8</td>
      <td>2</td>
      <td>1</td>
      <td>2019-04-30</td>
      <td>47</td>
    </tr>
    <tr>
      <th>1</th>
      <td>PL270116</td>
      <td>XXXXX</td>
      <td>C01</td>
      <td>M</td>
      <td>2015-05-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>0</td>
      <td>オールタイム</td>
      <td>10500</td>
      <td>通常</td>
      <td>5.083333</td>
      <td>5.0</td>
      <td>7</td>
      <td>3</td>
      <td>1</td>
      <td>2019-04-30</td>
      <td>47</td>
    </tr>
    <tr>
      <th>2</th>
      <td>OA974876</td>
      <td>XXXXX</td>
      <td>C01</td>
      <td>M</td>
      <td>2015-05-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>0</td>
      <td>オールタイム</td>
      <td>10500</td>
      <td>通常</td>
      <td>4.583333</td>
      <td>5.0</td>
      <td>6</td>
      <td>3</td>
      <td>1</td>
      <td>2019-04-30</td>
      <td>47</td>
    </tr>
    <tr>
      <th>3</th>
      <td>HD024127</td>
      <td>XXXXX</td>
      <td>C01</td>
      <td>F</td>
      <td>2015-05-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>0</td>
      <td>オールタイム</td>
      <td>10500</td>
      <td>通常</td>
      <td>4.833333</td>
      <td>4.5</td>
      <td>7</td>
      <td>2</td>
      <td>1</td>
      <td>2019-04-30</td>
      <td>47</td>
    </tr>
    <tr>
      <th>4</th>
      <td>HD661448</td>
      <td>XXXXX</td>
      <td>C03</td>
      <td>F</td>
      <td>2015-05-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>0</td>
      <td>ナイト</td>
      <td>6000</td>
      <td>通常</td>
      <td>3.916667</td>
      <td>4.0</td>
      <td>6</td>
      <td>1</td>
      <td>1</td>
      <td>2019-04-30</td>
      <td>47</td>
    </tr>
  </tbody>
</table>
</div>




```python
uselog_months.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>年月</th>
      <th>customer_id</th>
      <th>count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201804</td>
      <td>AS002855</td>
      <td>4</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201804</td>
      <td>AS009013</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>201804</td>
      <td>AS009373</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>201804</td>
      <td>AS015315</td>
      <td>6</td>
    </tr>
    <tr>
      <th>4</th>
      <td>201804</td>
      <td>AS015739</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
</div>




```python
year_months = list(uselog_months['年月'].unique())
uselog = pd.DataFrame()

for i in range(1, len(year_months)):
  tmp = uselog_months.loc[uselog_months['年月'] == year_months[i]].copy()
  tmp.rename(columns={'count': 'count_0'}, inplace=True)
  tmp_before = uselog_months.loc[uselog_months['年月'] == year_months[i - 1]].copy()
  del tmp_before['年月']
  tmp_before.rename(columns={'count': 'count_1'}, inplace=True)
  tmp = pd.merge(tmp, tmp_before, on='customer_id', how='left')
  uselog = pd.concat([uselog, tmp], ignore_index=True)
```


```python
uselog.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>年月</th>
      <th>customer_id</th>
      <th>count_0</th>
      <th>count_1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201805</td>
      <td>AS002855</td>
      <td>5</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201805</td>
      <td>AS009373</td>
      <td>4</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>201805</td>
      <td>AS015233</td>
      <td>7</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>201805</td>
      <td>AS015315</td>
      <td>3</td>
      <td>6.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>201805</td>
      <td>AS015739</td>
      <td>5</td>
      <td>7.0</td>
    </tr>
  </tbody>
</table>
</div>



### ノック : 42 大会前日の大会顧客データを作成しよう


```python
from dateutil.relativedelta import relativedelta
exit_customer = customer.loc[customer['is_deleted'] == 1].copy()

exit_customer['exit_date'] = None
exit_customer['end_date'] = pd.to_datetime(exit_customer['end_date'].copy())

for i in range(len(exit_customer)):
  col_id_exit_date = exit_customer.columns.get_loc('exit_date')
  col_id_end_date = exit_customer.columns.get_loc('end_date')
  exit_customer.iloc[i, col_id_exit_date] = exit_customer.iloc[i, col_id_end_date] - relativedelta(months=1)

exit_customer['年月'] = exit_customer['exit_date'].dt.strftime('%Y%m')
uselog['年月'] = uselog['年月'].astype(str)
exit_uselog = pd.merge(uselog, exit_customer, on=['customer_id', '年月'], how='left')
```


```python
len(uselog)
```




    33851




```python
exit_uselog.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>年月</th>
      <th>customer_id</th>
      <th>count_0</th>
      <th>count_1</th>
      <th>name</th>
      <th>class</th>
      <th>gender</th>
      <th>start_date</th>
      <th>end_date</th>
      <th>campaign_id</th>
      <th>...</th>
      <th>price</th>
      <th>campaign_name</th>
      <th>mean</th>
      <th>median</th>
      <th>max</th>
      <th>min</th>
      <th>routine_flg</th>
      <th>calc_date</th>
      <th>membership_period</th>
      <th>exit_date</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201805</td>
      <td>AS002855</td>
      <td>5</td>
      <td>4.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaT</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201805</td>
      <td>AS009373</td>
      <td>4</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaT</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>201805</td>
      <td>AS015233</td>
      <td>7</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaT</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>201805</td>
      <td>AS015315</td>
      <td>3</td>
      <td>6.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaT</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>201805</td>
      <td>AS015739</td>
      <td>5</td>
      <td>7.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaT</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 22 columns</p>
</div>



欠損値を除去します。


```python
exit_uselog = exit_uselog.dropna(subset=['name'])
print(len(exit_uselog))
print(len(exit_uselog['customer_id'].unique()))
exit_uselog.head()
```

    1104
    1104





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>年月</th>
      <th>customer_id</th>
      <th>count_0</th>
      <th>count_1</th>
      <th>name</th>
      <th>class</th>
      <th>gender</th>
      <th>start_date</th>
      <th>end_date</th>
      <th>campaign_id</th>
      <th>...</th>
      <th>price</th>
      <th>campaign_name</th>
      <th>mean</th>
      <th>median</th>
      <th>max</th>
      <th>min</th>
      <th>routine_flg</th>
      <th>calc_date</th>
      <th>membership_period</th>
      <th>exit_date</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>19</th>
      <td>201805</td>
      <td>AS055680</td>
      <td>3</td>
      <td>3.0</td>
      <td>XXXXX</td>
      <td>C01</td>
      <td>M</td>
      <td>2018-03-01</td>
      <td>2018-06-30</td>
      <td>CA1</td>
      <td>...</td>
      <td>10500.0</td>
      <td>通常</td>
      <td>3.000000</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>2018-06-30</td>
      <td>3.0</td>
      <td>2018-05-30 00:00:00</td>
    </tr>
    <tr>
      <th>57</th>
      <td>201805</td>
      <td>AS169823</td>
      <td>2</td>
      <td>3.0</td>
      <td>XX</td>
      <td>C01</td>
      <td>M</td>
      <td>2017-11-01</td>
      <td>2018-06-30</td>
      <td>CA1</td>
      <td>...</td>
      <td>10500.0</td>
      <td>通常</td>
      <td>3.000000</td>
      <td>3.0</td>
      <td>4.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>2018-06-30</td>
      <td>7.0</td>
      <td>2018-05-30 00:00:00</td>
    </tr>
    <tr>
      <th>110</th>
      <td>201805</td>
      <td>AS305860</td>
      <td>5</td>
      <td>3.0</td>
      <td>XXXX</td>
      <td>C01</td>
      <td>M</td>
      <td>2017-06-01</td>
      <td>2018-06-30</td>
      <td>CA1</td>
      <td>...</td>
      <td>10500.0</td>
      <td>通常</td>
      <td>3.333333</td>
      <td>3.0</td>
      <td>5.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>2018-06-30</td>
      <td>12.0</td>
      <td>2018-05-30 00:00:00</td>
    </tr>
    <tr>
      <th>128</th>
      <td>201805</td>
      <td>AS363699</td>
      <td>5</td>
      <td>3.0</td>
      <td>XXXXX</td>
      <td>C01</td>
      <td>M</td>
      <td>2018-02-01</td>
      <td>2018-06-30</td>
      <td>CA1</td>
      <td>...</td>
      <td>10500.0</td>
      <td>通常</td>
      <td>3.333333</td>
      <td>3.0</td>
      <td>5.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>2018-06-30</td>
      <td>4.0</td>
      <td>2018-05-30 00:00:00</td>
    </tr>
    <tr>
      <th>147</th>
      <td>201805</td>
      <td>AS417696</td>
      <td>1</td>
      <td>4.0</td>
      <td>XX</td>
      <td>C03</td>
      <td>F</td>
      <td>2017-09-01</td>
      <td>2018-06-30</td>
      <td>CA1</td>
      <td>...</td>
      <td>6000.0</td>
      <td>通常</td>
      <td>2.000000</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>2018-06-30</td>
      <td>9.0</td>
      <td>2018-05-30 00:00:00</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 22 columns</p>
</div>



### ノック : 43 継続顧客のデータを作成しよう


```python
conti_customer = customer.loc[customer['is_deleted'] == 0]
conti_uselog = pd.merge(uselog, conti_customer, on=['customer_id'], how='left')
```


```python
conti_uselog.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>年月</th>
      <th>customer_id</th>
      <th>count_0</th>
      <th>count_1</th>
      <th>name</th>
      <th>class</th>
      <th>gender</th>
      <th>start_date</th>
      <th>end_date</th>
      <th>campaign_id</th>
      <th>...</th>
      <th>class_name</th>
      <th>price</th>
      <th>campaign_name</th>
      <th>mean</th>
      <th>median</th>
      <th>max</th>
      <th>min</th>
      <th>routine_flg</th>
      <th>calc_date</th>
      <th>membership_period</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201805</td>
      <td>AS002855</td>
      <td>5</td>
      <td>4.0</td>
      <td>XXXX</td>
      <td>C03</td>
      <td>F</td>
      <td>2016-11-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>...</td>
      <td>ナイト</td>
      <td>6000.0</td>
      <td>通常</td>
      <td>4.500000</td>
      <td>5.0</td>
      <td>7.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>2019-04-30</td>
      <td>29.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201805</td>
      <td>AS009373</td>
      <td>4</td>
      <td>3.0</td>
      <td>XX</td>
      <td>C01</td>
      <td>F</td>
      <td>2015-11-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>...</td>
      <td>オールタイム</td>
      <td>10500.0</td>
      <td>通常</td>
      <td>5.083333</td>
      <td>5.0</td>
      <td>7.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>2019-04-30</td>
      <td>41.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>201805</td>
      <td>AS015233</td>
      <td>7</td>
      <td>NaN</td>
      <td>XXXXX</td>
      <td>C01</td>
      <td>M</td>
      <td>2018-05-13</td>
      <td>NaN</td>
      <td>CA2</td>
      <td>...</td>
      <td>オールタイム</td>
      <td>10500.0</td>
      <td>入会費半額</td>
      <td>7.545455</td>
      <td>7.0</td>
      <td>11.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>2019-04-30</td>
      <td>11.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>201805</td>
      <td>AS015315</td>
      <td>3</td>
      <td>6.0</td>
      <td>XXXXX</td>
      <td>C01</td>
      <td>M</td>
      <td>2015-07-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>...</td>
      <td>オールタイム</td>
      <td>10500.0</td>
      <td>通常</td>
      <td>4.833333</td>
      <td>5.0</td>
      <td>7.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>2019-04-30</td>
      <td>45.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>201805</td>
      <td>AS015739</td>
      <td>5</td>
      <td>7.0</td>
      <td>XXXXX</td>
      <td>C03</td>
      <td>M</td>
      <td>2017-06-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>...</td>
      <td>ナイト</td>
      <td>6000.0</td>
      <td>通常</td>
      <td>5.583333</td>
      <td>5.5</td>
      <td>8.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>2019-04-30</td>
      <td>22.0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 21 columns</p>
</div>




```python
print(len(conti_uselog))
```

    33851


欠損値を削除します。


```python
conti_uselog = conti_uselog.dropna(subset=['name'])
print(len(conti_uselog))
```

    27422


pandasのsampleメソッドで、fracオプションを使うと、任意の割合のデータをサンプリングする事が出来ます。


```python
conti_uselog = conti_uselog.sample(frac=1).reset_index(drop=True)
conti_uselog = conti_uselog.drop_duplicates(subset='customer_id')
print(len(conti_uselog))
conti_uselog.head()
```

    2842





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>年月</th>
      <th>customer_id</th>
      <th>count_0</th>
      <th>count_1</th>
      <th>name</th>
      <th>class</th>
      <th>gender</th>
      <th>start_date</th>
      <th>end_date</th>
      <th>campaign_id</th>
      <th>...</th>
      <th>class_name</th>
      <th>price</th>
      <th>campaign_name</th>
      <th>mean</th>
      <th>median</th>
      <th>max</th>
      <th>min</th>
      <th>routine_flg</th>
      <th>calc_date</th>
      <th>membership_period</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201807</td>
      <td>OA501813</td>
      <td>7</td>
      <td>4.0</td>
      <td>XXXX</td>
      <td>C03</td>
      <td>M</td>
      <td>2017-03-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>...</td>
      <td>ナイト</td>
      <td>6000.0</td>
      <td>通常</td>
      <td>5.000000</td>
      <td>5.0</td>
      <td>7.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>2019-04-30</td>
      <td>25.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201902</td>
      <td>HI140455</td>
      <td>3</td>
      <td>1.0</td>
      <td>XXXX</td>
      <td>C02</td>
      <td>F</td>
      <td>2016-01-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>...</td>
      <td>デイタイム</td>
      <td>7500.0</td>
      <td>通常</td>
      <td>4.750000</td>
      <td>5.0</td>
      <td>8.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2019-04-30</td>
      <td>39.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>201805</td>
      <td>AS069821</td>
      <td>2</td>
      <td>5.0</td>
      <td>XXXXXX</td>
      <td>C03</td>
      <td>F</td>
      <td>2015-05-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>...</td>
      <td>ナイト</td>
      <td>6000.0</td>
      <td>通常</td>
      <td>4.083333</td>
      <td>3.5</td>
      <td>7.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>2019-04-30</td>
      <td>47.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>201809</td>
      <td>HI270880</td>
      <td>6</td>
      <td>1.0</td>
      <td>XX</td>
      <td>C02</td>
      <td>F</td>
      <td>2015-09-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>...</td>
      <td>デイタイム</td>
      <td>7500.0</td>
      <td>通常</td>
      <td>4.000000</td>
      <td>3.0</td>
      <td>7.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2019-04-30</td>
      <td>43.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>201811</td>
      <td>AS929514</td>
      <td>5</td>
      <td>6.0</td>
      <td>XXXXXX</td>
      <td>C01</td>
      <td>M</td>
      <td>2017-04-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>...</td>
      <td>オールタイム</td>
      <td>10500.0</td>
      <td>通常</td>
      <td>5.500000</td>
      <td>5.0</td>
      <td>9.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>2019-04-30</td>
      <td>24.0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 21 columns</p>
</div>



退会顧客と継続顧客のデータの結合を行います。


```python
predict_data = pd.concat([conti_uselog, exit_uselog], ignore_index=True)
print(len(predict_data))
predict_data.head()
```

    3946





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>年月</th>
      <th>customer_id</th>
      <th>count_0</th>
      <th>count_1</th>
      <th>name</th>
      <th>class</th>
      <th>gender</th>
      <th>start_date</th>
      <th>end_date</th>
      <th>campaign_id</th>
      <th>...</th>
      <th>price</th>
      <th>campaign_name</th>
      <th>mean</th>
      <th>median</th>
      <th>max</th>
      <th>min</th>
      <th>routine_flg</th>
      <th>calc_date</th>
      <th>membership_period</th>
      <th>exit_date</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201807</td>
      <td>OA501813</td>
      <td>7</td>
      <td>4.0</td>
      <td>XXXX</td>
      <td>C03</td>
      <td>M</td>
      <td>2017-03-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>...</td>
      <td>6000.0</td>
      <td>通常</td>
      <td>5.000000</td>
      <td>5.0</td>
      <td>7.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>2019-04-30</td>
      <td>25.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201902</td>
      <td>HI140455</td>
      <td>3</td>
      <td>1.0</td>
      <td>XXXX</td>
      <td>C02</td>
      <td>F</td>
      <td>2016-01-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>...</td>
      <td>7500.0</td>
      <td>通常</td>
      <td>4.750000</td>
      <td>5.0</td>
      <td>8.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2019-04-30</td>
      <td>39.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>201805</td>
      <td>AS069821</td>
      <td>2</td>
      <td>5.0</td>
      <td>XXXXXX</td>
      <td>C03</td>
      <td>F</td>
      <td>2015-05-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>...</td>
      <td>6000.0</td>
      <td>通常</td>
      <td>4.083333</td>
      <td>3.5</td>
      <td>7.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>2019-04-30</td>
      <td>47.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>201809</td>
      <td>HI270880</td>
      <td>6</td>
      <td>1.0</td>
      <td>XX</td>
      <td>C02</td>
      <td>F</td>
      <td>2015-09-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>...</td>
      <td>7500.0</td>
      <td>通常</td>
      <td>4.000000</td>
      <td>3.0</td>
      <td>7.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2019-04-30</td>
      <td>43.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>201811</td>
      <td>AS929514</td>
      <td>5</td>
      <td>6.0</td>
      <td>XXXXXX</td>
      <td>C01</td>
      <td>M</td>
      <td>2017-04-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>...</td>
      <td>10500.0</td>
      <td>通常</td>
      <td>5.500000</td>
      <td>5.0</td>
      <td>9.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>2019-04-30</td>
      <td>24.0</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 22 columns</p>
</div>



### ノック : 44 予測する月の在籍期間を作成しよう


```python
predict_data['period'] = 0
predict_data['now_date'] = pd.to_datetime(predict_data['年月'], format="%Y%m")

predict_data['start_date'] = pd.to_datetime(predict_data['start_date'])

for i in range(len(predict_data)):
  delta = relativedelta(predict_data['now_date'][i], predict_data['start_date'][i])
  predict_data.iloc[i, predict_data.columns.get_loc('period')] = int(delta.years * 12 + delta.months)

predict_data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>年月</th>
      <th>customer_id</th>
      <th>count_0</th>
      <th>count_1</th>
      <th>name</th>
      <th>class</th>
      <th>gender</th>
      <th>start_date</th>
      <th>end_date</th>
      <th>campaign_id</th>
      <th>...</th>
      <th>mean</th>
      <th>median</th>
      <th>max</th>
      <th>min</th>
      <th>routine_flg</th>
      <th>calc_date</th>
      <th>membership_period</th>
      <th>exit_date</th>
      <th>period</th>
      <th>now_date</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201807</td>
      <td>OA501813</td>
      <td>7</td>
      <td>4.0</td>
      <td>XXXX</td>
      <td>C03</td>
      <td>M</td>
      <td>2017-03-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>...</td>
      <td>5.000000</td>
      <td>5.0</td>
      <td>7.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>2019-04-30</td>
      <td>25.0</td>
      <td>NaN</td>
      <td>16</td>
      <td>2018-07-01</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201902</td>
      <td>HI140455</td>
      <td>3</td>
      <td>1.0</td>
      <td>XXXX</td>
      <td>C02</td>
      <td>F</td>
      <td>2016-01-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>...</td>
      <td>4.750000</td>
      <td>5.0</td>
      <td>8.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2019-04-30</td>
      <td>39.0</td>
      <td>NaN</td>
      <td>37</td>
      <td>2019-02-01</td>
    </tr>
    <tr>
      <th>2</th>
      <td>201805</td>
      <td>AS069821</td>
      <td>2</td>
      <td>5.0</td>
      <td>XXXXXX</td>
      <td>C03</td>
      <td>F</td>
      <td>2015-05-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>...</td>
      <td>4.083333</td>
      <td>3.5</td>
      <td>7.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>2019-04-30</td>
      <td>47.0</td>
      <td>NaN</td>
      <td>36</td>
      <td>2018-05-01</td>
    </tr>
    <tr>
      <th>3</th>
      <td>201809</td>
      <td>HI270880</td>
      <td>6</td>
      <td>1.0</td>
      <td>XX</td>
      <td>C02</td>
      <td>F</td>
      <td>2015-09-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>...</td>
      <td>4.000000</td>
      <td>3.0</td>
      <td>7.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2019-04-30</td>
      <td>43.0</td>
      <td>NaN</td>
      <td>36</td>
      <td>2018-09-01</td>
    </tr>
    <tr>
      <th>4</th>
      <td>201811</td>
      <td>AS929514</td>
      <td>5</td>
      <td>6.0</td>
      <td>XXXXXX</td>
      <td>C01</td>
      <td>M</td>
      <td>2017-04-01</td>
      <td>NaN</td>
      <td>CA1</td>
      <td>...</td>
      <td>5.500000</td>
      <td>5.0</td>
      <td>9.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>2019-04-30</td>
      <td>24.0</td>
      <td>NaN</td>
      <td>19</td>
      <td>2018-11-01</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 24 columns</p>
</div>



### ノック : 45 欠損値を除去しよう

欠損値の確認をします


```python
predict_data.isna().sum()
```




    年月                      0
    customer_id             0
    count_0                 0
    count_1               267
    name                    0
    class                   0
    gender                  0
    start_date              0
    end_date             2842
    campaign_id             0
    is_deleted              0
    class_name              0
    price                   0
    campaign_name           0
    mean                    0
    median                  0
    max                     0
    min                     0
    routine_flg             0
    calc_date               0
    membership_period       0
    exit_date            2842
    period                  0
    now_date                0
    dtype: int64



これを見ると、count_1とend_date、exit_dateに欠損値が存在していることがわかります。end_dateとexit_dateは退会顧客しか値を持っていません。count_1に欠損値を持っているデータだけを削除します。


```python
predict_data = predict_data.dropna(subset=['count_1'])
predict_data.isna().sum()
```




    年月                      0
    customer_id             0
    count_0                 0
    count_1                 0
    name                    0
    class                   0
    gender                  0
    start_date              0
    end_date             2627
    campaign_id             0
    is_deleted              0
    class_name              0
    price                   0
    campaign_name           0
    mean                    0
    median                  0
    max                     0
    min                     0
    routine_flg             0
    calc_date               0
    membership_period       0
    exit_date            2627
    period                  0
    now_date                0
    dtype: int64



### ノック : 46 文字列型の変数を処理できるように整形しよう


```python
target_col = ['campaign_name', 'class_name', 'gender', 'count_1', 'routine_flg', 'period', 'is_deleted']
predict_data = predict_data[target_col]
predict_data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>campaign_name</th>
      <th>class_name</th>
      <th>gender</th>
      <th>count_1</th>
      <th>routine_flg</th>
      <th>period</th>
      <th>is_deleted</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>通常</td>
      <td>ナイト</td>
      <td>M</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>16</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>通常</td>
      <td>デイタイム</td>
      <td>F</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>37</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>通常</td>
      <td>ナイト</td>
      <td>F</td>
      <td>5.0</td>
      <td>1.0</td>
      <td>36</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>通常</td>
      <td>デイタイム</td>
      <td>F</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>36</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>通常</td>
      <td>オールタイム</td>
      <td>M</td>
      <td>6.0</td>
      <td>1.0</td>
      <td>19</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>



カテゴリカル変数を利用して、ダミー変数を作成します。get_dummyメソッドは文字列データを読み取って、それをダミー変数化してくれます。データ分析の現場ではしばしば利用されるとても便利な関数です。


```python
predict_data = pd.get_dummies(predict_data)
predict_data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>count_1</th>
      <th>routine_flg</th>
      <th>period</th>
      <th>is_deleted</th>
      <th>campaign_name_入会費半額</th>
      <th>campaign_name_入会費無料</th>
      <th>campaign_name_通常</th>
      <th>class_name_オールタイム</th>
      <th>class_name_デイタイム</th>
      <th>class_name_ナイト</th>
      <th>gender_F</th>
      <th>gender_M</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>4.0</td>
      <td>1.0</td>
      <td>16</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>37</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5.0</td>
      <td>1.0</td>
      <td>36</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>36</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>6.0</td>
      <td>1.0</td>
      <td>19</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




```python
predict_data.columns
```




    Index(['count_1', 'routine_flg', 'period', 'is_deleted', 'campaign_name_入会費半額',
           'campaign_name_入会費無料', 'campaign_name_通常', 'class_name_オールタイム',
           'class_name_デイタイム', 'class_name_ナイト', 'gender_F', 'gender_M'],
          dtype='object')



文字列をデータとして持つカラムだけダミー変数化されています。

本書では、わざわざ明示的に示されてなくてもわかるデータ（gender_Fの0か1がわかれば、gender_Mがわかるので、gender_Mは必要ない）を削除しています。
個人的にはあっても問題ないと思いますが、本書に従って削除します。


```python
del predict_data['campaign_name_通常']
del predict_data['class_name_ナイト']
del predict_data['gender_M']

predict_data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>count_1</th>
      <th>routine_flg</th>
      <th>period</th>
      <th>is_deleted</th>
      <th>campaign_name_入会費半額</th>
      <th>campaign_name_入会費無料</th>
      <th>class_name_オールタイム</th>
      <th>class_name_デイタイム</th>
      <th>gender_F</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>4.0</td>
      <td>1.0</td>
      <td>16</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>37</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5.0</td>
      <td>1.0</td>
      <td>36</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>36</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>6.0</td>
      <td>1.0</td>
      <td>19</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



### ノック : 47 決定木を用いて大会予測モデルを作成してみよう

実際に決定木のアルゴリズムを実行してみます。


```python
from sklearn.tree import DecisionTreeClassifier
import sklearn.model_selection

exit = predict_data.loc[predict_data['is_deleted'] == 1]

# 退会人数と同じ数だけサンプリング
conti = predict_data.loc[predict_data['is_deleted'] == 0].sample(len(exit))

X = pd.concat([exit, conti], ignore_index=True)
y = X['is_deleted']

# ターゲット変数を削除
del X['is_deleted']

X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X,y)

model = DecisionTreeClassifier(random_state=0)
model.fit(X_train, y_train)

# 予測値
y_test_pred = model.predict(X_test)

print(y_test_pred)
```

    [1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1.
     0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1.
     0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1.
     1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.
     0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1.
     0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1.
     1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
     1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.
     0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.
     1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0.
     1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1.
     0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1.
     1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0.
     1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
     1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1.
     1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0.
     1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.
     1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0.
     0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0.
     0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0.
     0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.
     0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1.]


正解データと予測されたデータの比較を行います。


```python
results_test = pd.DataFrame({
  'y_test': y_test,
  'y_pred': y_test_pred,
})
results_test.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>y_test</th>
      <th>y_pred</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>233</th>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1443</th>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>297</th>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>944</th>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1579</th>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>



### ノック : 48 予測モデルの評価を行い、モデルのチューニングをしてみよう

予測データと実際のデータが一致する割合を計算します。


```python
correct = len(results_test.loc[results_test['y_test'] == results_test['y_pred']])

data_count = len(results_test)

score_test = correct / data_count
print(score_test)
```

    0.8859315589353612



```python
model.score(X_test, y_test)
```




    0.8859315589353612




```python
model.score(X_train, y_train)
```




    0.976552598225602



学習用データを用いた場合が98％の一致率で、テスト用データの場合は89％となっています。学習用データに過剰適合（過学習）しています。過学習を防ぐためによく利用される深さの最大値を5に設定します。


```python
X = pd.concat([exit, conti], ignore_index=True)
y = X['is_deleted']

# ターゲット変数を削除
del X['is_deleted']

X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X,y)

model = DecisionTreeClassifier(random_state=0, max_depth=5)
model.fit(X_train, y_train)

print(model.score(X_test, y_test))
print(model.score(X_train, y_train))
```

    0.9201520912547528
    0.9226869455006337


モデルを作る際に考慮されていないテスト用データに対しても、同じ一致率を得ることが出来ました。

### ノック : 49 モデルに寄与している変数を確認しよう

決定木を利用している場合はfeature_importances_で変数の寄与度を得ることが出来ます。


```python
importance = pd.DataFrame({
  'feature_names': X.columns,
  'coefficient': model.feature_importances_
})
print(importance)
```

             feature_names  coefficient
    0              count_1     0.344085
    1          routine_flg     0.131421
    2               period     0.523109
    3  campaign_name_入会費半額     0.000000
    4  campaign_name_入会費無料     0.001385
    5    class_name_オールタイム     0.000000
    6     class_name_デイタイム     0.000000
    7             gender_F     0.000000


1ヶ月前の利用回数、定期利用、利用期間のそれぞれの寄与度が高いことがわかります。

### ノック : 50 顧客の退会を予測しよう

実際に未知のデータに対して予測してみます。予測したい適当なデータを作ります。


```python
count_1 = 3
routing_flg = 1
period = 10
campaign_name = '入会費無料'
class_name = 'オールタイム'
gender = 'M'
```

カテゴリカル変数を利用しているので、与えられた変数をカテゴライズ化します。


```python
if campaign_name == '入会費半額':
  campaign_name_list = [1,0]
elif campaign_name == '入会費無料':
  campaign_name_list = [0,1]
elif campaign_name == '通常':
  campaign_name_list = [0,0]
```


```python
if class_name == 'オールタイム':
  class_name_list = [1,0]
elif class_name == 'デイタイム':
  class_name_list = [0,1]
elif class_name == 'ナイト':
  class_name_list = [0,0]
```


```python
if gender == 'F':
  gender_list = [1]
elif gender == 'M':
  gender_list = [0]
```


```python
input_data = [count_1, routing_flg, period]
input_data.extend(campaign_name_list)
input_data.extend(class_name_list)
input_data.extend(gender_list)

input_data
```




    [3, 1, 10, 0, 1, 1, 0, 0]




```python
print(model.predict([input_data]))
print(model.predict_proba([input_data]))
```

    [1.]
    [[0.02484472 0.97515528]]


今回の決めたデータでは1で退会が予想されています。その確率も96％でかなり高い確率です。

## 関連記事
- [第1章 ウェブからの注文数を分析する10本ノック](/ml/data100/01/)
- [第2章 小売店のデータでデータ加工を行う10本ノック](/ml/data100/02/)
- [第3章 顧客の全体像を把握する10本ノック](/ml/data100/03/)
- [第4章 顧客の行動を予測する10本ノック](/ml/data100/04/)
- [第5章 顧客の退会を予測する10本ノック](/ml/data100/05/)
- [第6章 物流の最適ルートをコンサルティングする10本ノック](/ml/data100/06/)
- [第7章 ロジスティクスネットワークの最適設計を行う10本ノック](/ml/data100/07/)
- [第8章 数値シミュレーションで消費者行動を予測する10本ノック](/ml/data100/08/)
- [第9章 潜在顧客を把握するための画像認識10本ノック](/ml/data100/09/)
- [第10章 アンケート分析を行うための自然言語処理10本ノック](/ml/data100/10/)
