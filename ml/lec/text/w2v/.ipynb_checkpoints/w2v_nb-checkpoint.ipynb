{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec と doc2vec\n",
    "\n",
    "単語や文章を分散表現（意味が似たような単語や文章を似たようなベクトルとして表現）を取得します。\n",
    "\n",
    "### github\n",
    "- jupyter notebook形式のファイルは[こちら](https://github.com/hiroshi0530/wa-src/blob/master/ml/lec/text/w2v/w2v_nb.ipynb)\n",
    "\n",
    "### google colaboratory\n",
    "- google colaboratory で実行する場合は[こちら](https://colab.research.google.com/github/hiroshi0530/wa-src/blob/master/ml/lec/text/w2v/w2v_nb.ipynb)\n",
    "\n",
    "### 筆者の環境\n",
    "筆者のOSはmacOSです。LinuxやUnixのコマンドとはオプションが異なります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProductName:\tMac OS X\r\n",
      "ProductVersion:\t10.14.6\r\n",
      "BuildVersion:\t18G6032\r\n"
     ]
    }
   ],
   "source": [
    "!sw_vers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.5\r\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本的なライブラリをインポートしそのバージョンを確認しておきます。tensorflowとkerasuのversionも確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version : 3.3.2\n",
      "scipy version : 1.5.2\n",
      "numpy version : 1.18.5\n",
      "tensorflow version :  2.3.1\n",
      "keras version :  2.4.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print('matplotlib version :', matplotlib.__version__)\n",
    "print('scipy version :', scipy.__version__)\n",
    "print('numpy version :', np.__version__)\n",
    "print('tensorflow version : ', tf.__version__)\n",
    "print('keras version : ', keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストデータの取得\n",
    "\n",
    "著作権の問題がない青空文庫からすべての作品をダウンロードしてきます。gitがかなり重いので、最新の履歴だけを取得します。\n",
    "\n",
    "```bash\n",
    "git clone --depth 1 https://github.com/aozorabunko/aozorabunko.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際のファイルはcardsにzip形式として保存されているようです。ディレクトリの個数を確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   19636\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./aozorabunko/cards/* | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zipファイルだけzipsに移動させます。\n",
    "\n",
    "```bash\n",
    "find ./aozorabunko/cards/ -name *.zip | xargs -I{} cp {} -t ./zips/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000_ruby_2956.zip\r\n",
      "1001_ruby_2229.zip\r\n",
      "1002_ruby_20989.zip\r\n",
      "1003_ruby_2008.zip\r\n",
      "1004_ruby_2053.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./zips/ | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   16444\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./zips/ | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "となり、16444個のzipファイルがある事が分かります。こちらをすべて解凍し、ディレクトリを移動させます。\n",
    "\n",
    "```bash\n",
    "for i in `ls`; do [[ ${i##*.} == zip ]] && unzip -o $i -d ../texts/; done\n",
    "```\n",
    "\n",
    "これで、textｓというディレクトリにすべての作品のテキストファイルがインストールされました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miyazawa_kenji_zenshu.txt\r\n",
      "miyazawa_kenji_zenshuno_kankoni_saishite.txt\r\n",
      "miyazawa_kenjino_sekai.txt\r\n",
      "miyazawa_kenjino_shi.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./texts/ | grep miyazawa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ginga_tetsudono_yoru.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./texts/ | grep ginga_tetsudo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "となり、宮沢賢治関連の作品も含まれていることが分かります。銀河鉄道の夜もあります。\n",
    "\n",
    "## 銀河鉄道の夜を使ったword2vec\n",
    "\n",
    "今回はすべてのテキストファイルを対象にするには時間がかかるので、同じ岩手県出身の、高校の先輩でもある宮沢賢治の作品を例に取りword2vecを試してみます。\n",
    "しかし、ファイルの中身を見てみると、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "��͓S���̖�\r",
      "\r\n",
      "�{�򌫎�\r",
      "\r\n",
      "\r",
      "\r\n",
      "-------------------------------------------------------\r",
      "\r\n",
      "�y�e�L�X�g���Ɍ����L���ɂ��āz\r",
      "\r\n",
      "\r",
      "\r\n",
      "�s�t�F���r\r",
      "\r\n",
      "�i��j�k�\\���s�������ӂ��t\r",
      "\r\n",
      "\r",
      "\r\n",
      "�m���n�F���͎Ғ��@��ɊO���̐�����A�T�_�̈ʒu�̎w��\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head ./texts/ginga_tetsudono_yoru.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shift_JIS (CRLF)\r\n"
     ]
    }
   ],
   "source": [
    "!nkf --guess ./texts/ginga_tetsudono_yoru.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "となりshift_jisで保存されていることが分かります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nkf -w ./texts/ginga_tetsudono_yoru.txt > ginga.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "と、ディレクトリを変更し、ファイル名も変更します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "銀河鉄道の夜\r",
      "\r\n",
      "宮沢賢治\r",
      "\r\n",
      "\r",
      "\r\n",
      "-------------------------------------------------------\r",
      "\r\n",
      "【テキスト中に現れる記号について】\r",
      "\r\n",
      "\r",
      "\r\n",
      "《》：ルビ\r",
      "\r\n",
      "（例）北十字《きたじふじ》\r",
      "\r\n",
      "\r",
      "\r\n",
      "［＃］：入力者注　主に外字の説明や、傍点の位置の指定\r",
      "\r\n",
      "　　　（数字は、JIS X 0213の面区点番号またはUnicode、底本のページと行数）\r",
      "\r\n",
      "（例）※［＃小書き片仮名ヰ、155-15］\r",
      "\r\n",
      "\r",
      "\r\n",
      "　［＃（…）］：訓点送り仮名\r",
      "\r\n",
      "　（例）僕［＃（ん）］とこ\r",
      "\r\n",
      "-------------------------------------------------------\r",
      "\r\n",
      "\r",
      "\r\n",
      "［＃７字下げ］一、午后の授業［＃「一、午后の授業」は中見出し］\r",
      "\r\n",
      "\r",
      "\r\n",
      "「ではみなさんは、さういふふうに川だと云はれたり、乳の流れたあとだと云はれたりしてゐたこのぼんやりと白いものがほんたうは何かご承知ですか。」先生は、黒板に吊した大きな黒い星座の図の、上から下へ白くけぶった銀河帯のやうなところを指しながら、みんなに問をかけました。\r",
      "\r\n",
      "カムパネルラが手をあげました。それから四五人手をあげました。ジョバンニも手をあげやうとして、急いでそのまゝやめました。たしかにあれがみんな星だと、いつか雑誌で読んだのでしたが、このごろはジョバンニはまるで毎日教室でもねむく、本を読むひまも読む本もないので、なんだかどんなこともよくわからないといふ気持ちがするのでした。\r",
      "\r\n",
      "ところが先生は早くもそれを見附けたのでした。\r",
      "\r\n",
      "「ジョバンニさん。あなたはわかってゐるのでせう。」\r",
      "\r\n",
      "ジョバンニは勢よく立ちあがりましたが、立って見るともうはっきりとそれを答へることができないのでした。ザネリが前の席からふりかへって、ジョバンニを見てくすっとわらひました。ジョバンニはもうどぎまぎしてまっ赤になってしまひました。先生がまた云ひました。\r",
      "\r\n",
      "「大きな望遠鏡で銀河をよっく調べると銀河は大体何でせう。」\r",
      "\r\n",
      "cat: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!cat ginga.txt | head -n 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ジョバンニはそのカムパネルラはもうあの銀河のはづれにしかゐないといふやうな気がしてしかたなかったのです。\r",
      "\r\n",
      "けれどもみんなはまだ、どこかの波の間から、\r",
      "\r\n",
      "「ぼくずゐぶん泳いだぞ。」と云ひながらカムパネルラが出て来るか或ひはカムパネルラがどこかの人の知らない洲にでも着いて立ってゐて誰かの来るのを待ってゐるかといふやうな気がして仕方ないらしいのでした。けれども俄かにカムパネルラのお父さんがきっぱり云ひました。\r",
      "\r\n",
      "「もう駄目です。落ちてから四十五分たちましたから。」\r",
      "\r\n",
      "ジョバンニは思はずか〔け〕よって博士の前に立って、ぼくはカムパネルラの行った方を知ってゐますぼくはカムパネルラといっしょに歩いてゐたのですと云はうとしましたがもうのどがつまって何とも云へませんでした。すると博士はジョバンニが挨拶に来たとでも思ったものですか　しばらくしげしげジョバンニを見てゐましたが\r",
      "\r\n",
      "「あなたはジョバンニさんでしたね。どうも今晩はありがたう。」と叮ねいに云ひました。\r",
      "\r\n",
      "　ジョバンニは何も云へずにたゞおじぎをしました。\r",
      "\r\n",
      "「あなたのお父さんはもう帰ってゐますか。」博士は堅く時計を握ったまゝまたきゝました。\r",
      "\r\n",
      "「いゝえ。」ジョバンニはかすかに頭をふりました。\r",
      "\r\n",
      "「どうしたのかなあ、ぼくには一昨日大へん元気な便りがあったんだが。今日あ〔〕たりもう着くころなんだが。船が遅れたんだな。ジョバンニさん。あした放課后みなさんとうちへ遊びに来てくださいね。」\r",
      "\r\n",
      "さう云ひながら博士は〔〕また川下の銀河のいっぱいにうつった方へじっと眼を送りました。ジョバンニはもういろいろなことで胸がいっぱいでなんにも云へずに博士の前をはなれて早くお母さんに牛乳を持って行ってお父さんの帰ることを知らせやうと思ふともう一目散に河原を街の方へ走りました。\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "底本：「【新】校本宮澤賢治全集　第十一巻　童話※［＃ローマ数字4、1-13-24］　本文篇」筑摩書房\r",
      "\r\n",
      "　　　1996（平成8）年1月25日初版第1刷発行\r",
      "\r\n",
      "※底本のテキストは、著者草稿によります。\r",
      "\r\n",
      "※底本では校訂及び編者による説明を「〔　〕」、削除を「〔〕」で表示しています。\r",
      "\r\n",
      "※「カムパネルラ」と「カンパネルラ」の混在は、底本通りです。\r",
      "\r\n",
      "※底本は新字旧仮名づかいです。なお拗音、促音の小書きは、底本通りです。\r",
      "\r\n",
      "入力：砂場清隆\r",
      "\r\n",
      "校正：北川松生\r",
      "\r\n",
      "2016年6月10日作成\r",
      "\r\n",
      "青空文庫作成ファイル：\r",
      "\r\n",
      "このファイルは、インターネットの図書館、青空文庫（http://www.aozora.gr.jp/）で作られました。入力、校正、制作にあたったのは、ボランティアの皆さんです。\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ginga.txt | tail -n 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "となり、ファイルの先頭と、末尾に参考情報が載っているほかは、ちゃんとテキストとしてデータが取れている模様です。\n",
    "先ず、この辺の前処理を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open('ginga.txt', mode='r') as f:\n",
    "  all_sentence = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全角、半角の空白、改行コード、縦線(|)をすべて削除します。正規表現を利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentence = all_sentence.replace(\" \", \"\").replace(\"　\",\"\").replace(\"\\n\",\"\").replace(\"|\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "《》で囲まれたルビの部分を削除します。正規表現を利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentence = re.sub(\"《[^》]+》\", \"\", all_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------の部分で分割を行い、2番目の要素を取得します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_sentence = re.split(\"\\-{8,}\", all_sentence)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "。で分割し、文ごとにリストに格納します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['［＃７字下げ］一、午后の授業［＃「一、午后の授業」は中見出し］「ではみなさんは、さういふふうに川だと云はれたり、乳の流れたあとだと云はれたりしてゐたこのぼんやりと白いものがほんたうは何かご承知ですか。',\n",
       " '」先生は、黒板に吊した大きな黒い星座の図の、上から下へ白くけぶった銀河帯のやうなところを指しながら、みんなに問をかけました。',\n",
       " 'カムパネルラが手をあげました。',\n",
       " 'それから四五人手をあげました。',\n",
       " 'ジョバンニも手をあげやうとして、急いでそのまゝやめました。']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list = all_sentence.split(\"。\")\n",
    "sentence_list = [ s + \"。\" for s in sentence_list]\n",
    "sentence_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初の文は不要なので削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['」先生は、黒板に吊した大きな黒い星座の図の、上から下へ白くけぶった銀河帯のやうなところを指しながら、みんなに問をかけました。',\n",
       " 'カムパネルラが手をあげました。',\n",
       " 'それから四五人手をあげました。',\n",
       " 'ジョバンニも手をあげやうとして、急いでそのまゝやめました。',\n",
       " 'たしかにあれがみんな星だと、いつか雑誌で読んだのでしたが、このごろはジョバンニはまるで毎日教室でもねむく、本を読むひまも読む本もないので、なんだかどんなこともよくわからないといふ気持ちがするのでした。']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list = sentence_list[1:]\n",
    "sentence_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "となり、不要な部分を削除し、一文ごとにリストに格納できました。前処理は終了です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## janomeによる形態素解析\n",
    "\n",
    "janomeは日本語の文章を形態素ごとに分解する事が出来るツールです。同じようなツールとして、MecabやGinzaなどがあります。一長一短があると思いますが、ここではjanomeを利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['」', '先生', 'は', '、', '黒板']\n",
      "[['」', '先生', 'は', '、', '黒板', 'に', '吊し', 'た', '大きな', '黒い', '星座', 'の', '図', 'の', '、', '上', 'から', '下', 'へ', '白く', 'けぶっ', 'た', '銀河', '帯', 'の', 'やう', 'な', 'ところ', 'を', '指し', 'ながら', '、', 'みんな', 'に', '問', 'を', 'かけ', 'まし', 'た', '。'], ['カムパネルラ', 'が', '手', 'を', 'あげ', 'まし', 'た', '。'], ['それから', '四', '五', '人', '手', 'を', 'あげ', 'まし', 'た', '。'], ['ジョバンニ', 'も', '手', 'を', 'あげ', 'やう', 'として', '、', '急い', 'で', 'その', 'ま', 'ゝ', 'やめ', 'まし', 'た', '。'], ['たしかに', 'あれ', 'が', 'みんな', '星', 'だ', 'と', '、', 'いつか', '雑誌', 'で', '読ん', 'だ', 'の', 'でし', 'た', 'が', '、', 'このごろ', 'は', 'ジョバンニ', 'は', 'まるで', '毎日', '教室', 'で', 'も', 'ねむく', '、', '本', 'を', '読む', 'ひま', 'も', '読む', '本', 'も', 'ない', 'ので', '、', 'なんだか', 'どんな', 'こと', 'も', 'よく', 'わから', 'ない', 'といふ', '気持ち', 'が', 'する', 'の', 'でし', 'た', '。']]\n"
     ]
    }
   ],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "word_list = []\n",
    "word_per_sentence_list = []\n",
    "for sentence in sentence_list:\n",
    "  word_list.extend(list(t.tokenize(sentence, wakati=True)))\n",
    "  word_per_sentence_list.append(list(t.tokenize(sentence, wakati=True)))\n",
    "  \n",
    "print(word_list[:5])\n",
    "print(word_per_sentence_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単語のカウント\n",
    "\n",
    "単語のカウントを行い、出現頻度の高いベスト10を抽出してみます。名詞のみに限定した方が良かったかもしれません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('の', 1258),\n",
       " ('。', 1126),\n",
       " ('た', 1019),\n",
       " ('、', 1002),\n",
       " ('て', 864),\n",
       " ('に', 712),\n",
       " ('は', 665),\n",
       " ('を', 573),\n",
       " ('が', 514),\n",
       " ('まし', 460)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "count = collections.Counter(word_list)\n",
    "count.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensimに含まれるword2vecを用いた学習\n",
    "\n",
    "word2vecを用いて、word_listの分散表現を取得します。使い方はいくらでも検索できますので、ここでは割愛します。単語のリストを渡せば、ほぼ自動的に分散表現を作ってくれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(word_list, size=100, min_count=5, window=5, iter=20, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分散行列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16148804, -0.22206312,  0.01514053, ..., -0.16706243,\n",
       "         0.02306935, -0.11215351],\n",
       "       [ 0.02985465, -0.29097486,  0.23488192, ..., -0.07510845,\n",
       "        -0.20701611,  0.02986565],\n",
       "       [ 0.2021041 , -0.38445342,  0.54651535, ..., -0.05142045,\n",
       "        -0.10068199, -0.35940263],\n",
       "       ...,\n",
       "       [-0.06465647, -0.08360212, -0.01539595, ...,  0.09340496,\n",
       "        -0.07631809, -0.06988764],\n",
       "       [-0.07460806, -0.14879605, -0.01712933, ...,  0.0950413 ,\n",
       "        -0.09897593, -0.08147384],\n",
       "       [-0.00394372, -0.00091575, -0.00134183, ..., -0.00416549,\n",
       "         0.00147533,  0.0028519 ]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分散行列の形状確認\n",
    "\n",
    "443個の単語について、100次元のベクトルが生成されました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(607, 100)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全単語数は、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2687"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ですが、word2vecのmin_countを5にしているので、その文単語数が少なくなっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['の', 'た', 'し', '。', 'て', 'っ', '、', 'ま', 'い', 'な']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index2word[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.16148804, -0.22206312,  0.01514053,  0.02530331, -0.0891323 ,\n",
       "       -0.02909604,  0.18136026, -0.13019255,  0.1709021 , -0.11549854,\n",
       "        0.06013799, -0.36910063, -0.11393891, -0.04352104, -0.13912413,\n",
       "        0.20084661, -0.1191317 ,  0.10199364, -0.18353897,  0.20762777,\n",
       "        0.17698285, -0.18865503,  0.22168973, -0.12109254,  0.32947117,\n",
       "        0.07671156, -0.04303527,  0.21375673,  0.03085341,  0.08144953,\n",
       "        0.07714482, -0.07816965, -0.14268254, -0.15070039, -0.023989  ,\n",
       "       -0.25029692,  0.08508571, -0.12193882, -0.04827277, -0.00138727,\n",
       "        0.204026  , -0.05619547,  0.29465854,  0.19908844,  0.2384443 ,\n",
       "       -0.13478845,  0.05493325,  0.01110022,  0.07890843,  0.01807013,\n",
       "       -0.07020055,  0.09857967,  0.05283378, -0.33650437,  0.33631778,\n",
       "       -0.27991253, -0.05473338,  0.03647203, -0.34254655,  0.3391084 ,\n",
       "       -0.15791827,  0.3811276 , -0.19020417,  0.02860726, -0.09663328,\n",
       "       -0.11288399, -0.07717905, -0.25668275, -0.04397521, -0.34596565,\n",
       "       -0.42313537, -0.06878683, -0.38193542,  0.27987826, -0.08327626,\n",
       "        0.3406955 ,  0.23332499, -0.01711988, -0.24273866, -0.15768948,\n",
       "       -0.15057772, -0.16240793,  0.00661547,  0.11898061,  0.30435482,\n",
       "       -0.01839367, -0.22801025, -0.03116721, -0.00263738, -0.15558764,\n",
       "        0.00499189, -0.00182506,  0.04880338, -0.43814376, -0.24285571,\n",
       "        0.40474695,  0.04648104, -0.16706243,  0.02306935, -0.11215351],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.16148804, -0.22206312,  0.01514053,  0.02530331, -0.0891323 ,\n",
       "       -0.02909604,  0.18136026, -0.13019255,  0.1709021 , -0.11549854,\n",
       "        0.06013799, -0.36910063, -0.11393891, -0.04352104, -0.13912413,\n",
       "        0.20084661, -0.1191317 ,  0.10199364, -0.18353897,  0.20762777,\n",
       "        0.17698285, -0.18865503,  0.22168973, -0.12109254,  0.32947117,\n",
       "        0.07671156, -0.04303527,  0.21375673,  0.03085341,  0.08144953,\n",
       "        0.07714482, -0.07816965, -0.14268254, -0.15070039, -0.023989  ,\n",
       "       -0.25029692,  0.08508571, -0.12193882, -0.04827277, -0.00138727,\n",
       "        0.204026  , -0.05619547,  0.29465854,  0.19908844,  0.2384443 ,\n",
       "       -0.13478845,  0.05493325,  0.01110022,  0.07890843,  0.01807013,\n",
       "       -0.07020055,  0.09857967,  0.05283378, -0.33650437,  0.33631778,\n",
       "       -0.27991253, -0.05473338,  0.03647203, -0.34254655,  0.3391084 ,\n",
       "       -0.15791827,  0.3811276 , -0.19020417,  0.02860726, -0.09663328,\n",
       "       -0.11288399, -0.07717905, -0.25668275, -0.04397521, -0.34596565,\n",
       "       -0.42313537, -0.06878683, -0.38193542,  0.27987826, -0.08327626,\n",
       "        0.3406955 ,  0.23332499, -0.01711988, -0.24273866, -0.15768948,\n",
       "       -0.15057772, -0.16240793,  0.00661547,  0.11898061,  0.30435482,\n",
       "       -0.01839367, -0.22801025, -0.03116721, -0.00263738, -0.15558764,\n",
       "        0.00499189, -0.00182506,  0.04880338, -0.43814376, -0.24285571,\n",
       "        0.40474695,  0.04648104, -0.16706243,  0.02306935, -0.11215351],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.__getitem__(\"の\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cos類似度による単語抽出\n",
    "\n",
    "ベクトルの内積を計算することにより、指定した単語に類似した単語をその$\\cos$の値と一緒に抽出する事ができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('※', 0.9979097843170166), ('文', 0.9976733326911926), ('月', 0.9975945353507996), ('球', 0.9975260496139526), ('仕', 0.9974381327629089), ('底', 0.9974122047424316), ('教', 0.9973639249801636), ('右', 0.9972889423370361), ('元', 0.9970399141311646), ('牛', 0.9969346523284912)]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'ジョバンニ' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-f3786842deb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"本\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ジョバンニ\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'ジョバンニ' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"銀河\"))\n",
    "print(model.wv.most_similar(\"本\"))\n",
    "print(model.wv.most_similar(\"ジョバンニ\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語ベクトルによる演算\n",
    "\n",
    "足し算するにはpositiveメソッドを引き算にはnegativeメソッドを利用します。\n",
    "\n",
    "まず、銀河＋男を計算します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=[\"銀河\", \"ジョバンニ\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に銀河＋ジョバンニー家を計算します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=[\"銀河\", \"ジョバンニ\"], negative=[\"家\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec\n",
    "\n",
    "文章毎にタグ付けされたTaggedDocumentを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "tagged_doc_list = []\n",
    "\n",
    "for i, sentence in enumerate(word_per_sentence_list):\n",
    "  tagged_doc_list.append(TaggedDocument(sentence, [i]))\n",
    "\n",
    "print(tagged_doc_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(documents=tagged_doc_list, vector_size=100, min_count=5, window=5, epochs=20, dm=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_per_sentence_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.docvecs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most_similarで類似度が高い文章のIDと類似度を取得することが出来ます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.docvecs.most_similar(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.docvecs.most_similar(0):\n",
    "  print(word_per_sentence_list[p[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感覚的ですが、似たような文章が抽出されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
