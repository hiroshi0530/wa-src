## top template

### github
- jupyter notebook形式のファイルは[こちら](https://github.com/hiroshi0530/wa-src/blob/master/article/library/scipy/template/template_nb.ipynb)

### google colaboratory
- google colaboratory で実行する場合は[こちら](https://colab.research.google.com/github/hiroshi0530/wa-src/blob/master/article/library/scipy/template/template_nb.ipynb)


### github
- jupyter notebook形式のファイルは[こちら](https://github.com/hiroshi0530/wa-src/blob/master/rec/graph/01/01_nb.ipynb)

### google colaboratory
- google colaboratory で実行する場合は[こちら](https://colab.research.google.com/github/hiroshi0530/wa-src/blob/master/rec/graph/01/01_nb.ipynb)


### 実行環境
OSはmacOSです。LinuxやUnixのコマンドとはオプションが異なりますので注意してください。


```python
!sw_vers
```

    ProductName:		macOS
    ProductVersion:		13.5.1
    BuildVersion:		22G90



```python
!python -V
```

    Python 3.9.17


pandasのテーブルを見やすいようにHTMLのテーブルにCSSの設定を行います。


```python
from IPython.core.display import HTML

style = """
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
        padding: 5px;
    }

    .dataframe tbody tr th {
        vertical-align: top;
        padding: 5px;
    }

    .dataframe tbody tr:hover {
        background-color: #ffff99;
    }

    .dataframe {
        background-color: white;
        color: black;
        font-size: 16px;
    }

</style>
"""
HTML(style)
```

基本的なライブラリをインポートし watermark を利用してそのバージョンを確認しておきます。
ついでに乱数のseedの設定をします。


```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'
```


```python
import random

import scipy
import numpy as np

import matplotlib
import matplotlib.pyplot as plt

seed = 123
random_state = 123

random.seed(seed)
np.random.seed(seed)


from watermark import watermark

print(watermark(python=True, watermark=True, iversions=True, globals_=globals()))
```

    Python implementation: CPython
    Python version       : 3.9.17
    IPython version      : 8.17.2
    
    scipy     : 1.11.2
    numpy     : 1.25.2
    matplotlib: 3.8.1
    
    Watermark: 2.4.3
    


## 特異値分解（SVD）

特異値分解は、任意の行列 $A$ を以下のように分解する方法です：

$$ A = U \Sigma V^T $$

ここで、

- $A$ は $m \times n$ の行列
- $U$ は $m \times m$ の直交行列（左特異ベクトル）
- $\Sigma$ は $m \times n$ の対角行列（特異値を含む）
- $V$ は $n \times n$ の直交行列（右特異ベクトル）

行列 $A$ の特異値は、行列 $A^T A$ または $A A^T$ の固有値の平方根です。

例えば、行列 $A$ が次のようであるとします：

$$ A = \begin{pmatrix} 1 & 0 & 0 & 0 & 2 \\ 0 & 0 & 3 & 0 & 0 \end{pmatrix} $$

SVDを適用すると、次のように分解されます：

$$ U = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}, \quad \Sigma = \begin{pmatrix} 3 & 0 & 0 & 0 & 0 \\ 0 & 2 & 0 & 0 & 0 \end{pmatrix}, \quad V = \begin{pmatrix} 0 & 0 & 1 & 0 & 0 \\ 1 & 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 & 1 \end{pmatrix} $$

## 主成分分析（PCA）

主成分分析は、データの分散を最大化する方向に射影を行うことで、次元削減を行う手法です。PCAの基本的なステップは次の通りです：

1. データ行列 $X$ をセンタリングする（各変数の平均を引く）：

   $$ X_{\text{centered}} = X - \bar{X} $$

2. 共分散行列 $\Sigma$ を計算する：

   $$ \Sigma = \frac{1}{n-1} X_{\text{centered}}^T X_{\text{centered}} $$

3. 共分散行列 $\Sigma$ の固有値分解を行う：

   $$ \Sigma = P \Lambda P^T $$

   ここで、$P$ は固有ベクトルの行列、$\Lambda$ は固有値の対角行列です。

4. 主成分を選択する。固有値が大きい順に対応する固有ベクトルを選びます。

5. データを新しい基底に変換する：

   $$ X_{\text{new}} = X_{\text{centered}} P $$

例として、2次元データを考えます：

$$ X = \begin{pmatrix} 2.5 & 2.4 \\ 0.5 & 0.7 \\ 2.2 & 2.9 \\ 1.9 & 2.2 \\ 3.1 & 3.0 \\ 2.3 & 2.7 \\ 2.0 & 1.6 \\ 1.0 & 1.1 \\ 1.5 & 1.6 \\ 1.1 & 0.9 \end{pmatrix} $$

まず、各列の平均を引いてセンタリングします：

$$ X_{\text{centered}} = X - \bar{X} = \begin{pmatrix} 2.5 - 1.81 & 2.4 - 1.91 \\ 0.5 - 1.81 & 0.7 - 1.91 \\ 2.2 - 1.81 & 2.9 - 1.91 \\ 1.9 - 1.81 & 2.2 - 1.91 \\ 3.1 - 1.81 & 3.0 - 1.91 \\ 2.3 - 1.81 & 2.7 - 1.91 \\ 2.0 - 1.81 & 1.6 - 1.91 \\ 1.0 - 1.81 & 1.1 - 1.91 \\ 1.5 - 1.81 & 1.6 - 1.91 \\ 1.1 - 1.81 & 0.9 - 1.91 \end{pmatrix} $$

次に共分散行列を計算します：

$$ \Sigma = \frac{1}{9} X_{\text{centered}}^T X_{\text{centered}} $$

固有値分解を行い、主成分を選びます。これにより、データを低次元に変換することができます。

SVDとPCAは密接に関連しており、特にPCAを行う際にSVDを使用することが一般的です。PCAでは、データ行列の共分散行列の固有ベクトルと固有値を求めますが、これはSVDを適用することで効率的に計算できます。

## 特異値分解（SVD）の意味

特異値分解（Singular Value Decomposition, SVD）は、任意の行列を分解して、その行列の構造を理解しやすくするための数学的手法です。特に、次のような用途で有効です：

1. **次元削減**：高次元データを低次元に圧縮する際に使われます。データの情報をできるだけ保持しながら、データの次元を削減できます。
2. **ノイズ除去**：データからノイズを除去して、重要な信号を抽出するのに役立ちます。
3. **行列の近似**：大規模な行列を低ランクの行列で近似することで、計算の効率化を図ります。
4. **画像圧縮**：画像データを圧縮して、データ量を削減するために使われます。

## 特異値分解の具体的な使用例

### 1. 次元削減

特異値分解は、データの次元削減に頻繁に使用されます。例えば、テキストデータの解析で使われる「潜在意味解析（Latent Semantic Analysis, LSA）」は、文書の行列に対してSVDを適用して、単語と文書の関係を低次元空間に写像する技術です。

### 2. ノイズ除去

例えば、画像データのノイズを除去する際にSVDを利用できます。画像を行列として表現し、その行列にSVDを適用して、ノイズ成分を低ランク近似で除去します。

### 3. 画像圧縮

画像圧縮の例を具体的に見てみましょう。以下はSVDを使った画像圧縮の手順です：

1. **画像を行列に変換**：カラー画像をグレースケール画像に変換し、各ピクセルの輝度値を行列の要素として表現します。
2. **SVDを適用**：行列に対してSVDを行い、$A = U \Sigma V^T$ の形式に分解します。
3. **低ランク近似**：特異値が大きい上位$k$個の値だけを使って、元の行列を近似します。このとき、低ランク行列 $A_k = U_k \Sigma_k V_k^T$ を構築します。
4. **画像の再構成**：低ランク近似行列から画像を再構成し、元の画像と比較します。特異値の数$k$が少ないほど画像はぼやけますが、データ量は大幅に減少します。

具体的な例として、以下の手順で説明します：

1. **元の画像行列 $A$**：

   $$
   A = \begin{pmatrix}
   255 & 230 & 200 \\
   230 & 200 & 180 \\
   200 & 180 & 150
   \end{pmatrix}
   $$

2. **SVDを適用**：

   $$
   A = U \Sigma V^T
   $$

   ここで、

   $$
   U = \begin{pmatrix}
   0.577 & 0.707 & 0.408 \\
   0.577 & 0.000 & -0.816 \\
   0.577 & -0.707 & 0.408
   \end{pmatrix}, \quad
   \Sigma = \begin{pmatrix}
   450 & 0 & 0 \\
   0 & 20 & 0 \\
   0 & 0 & 10
   \end{pmatrix}, \quad
   V = \begin{pmatrix}
   0.577 & 0.707 & 0.408 \\
   0.577 & 0.000 & -0.816 \\
   0.577 & -0.707 & 0.408
   \end{pmatrix}
   $$

3. **低ランク近似**：

   特異値の上位2つだけを使って近似行列を作成します：

   $$
   \Sigma_2 = \begin{pmatrix}
   450 & 0 & 0 \\
   0 & 20 & 0 \\
   0 & 0 & 0
   \end{pmatrix}
   $$

   これにより、低ランク近似行列 $A_2$ は次のようになります：

   $$
   A_2 = U \Sigma_2 V^T
   $$

4. **再構成画像**：

   この近似行列 $A_2$ を使って再構成した画像は、元の画像に比べてデータ量が減少しつつも、視覚的にほとんど変わらない品質を保ちます。

これらのステップを実行することで、画像のデータ圧縮が可能となります。このように、SVDはデータの構造を解析し、情報の損失を最小限に抑えながらデータの次元削減や圧縮を行う強力なツールです。

## 主成分分析（PCA）の意味

主成分分析（Principal Component Analysis, PCA）は、多次元データセットの次元を削減するための統計手法です。PCAは、データの分散を最大化する方向（主成分）にデータを射影することで、新しい基底を見つけます。これにより、元のデータの最も重要な情報を保持しながら、次元削減が行えます。

PCAの主な目的は次のとおりです：
1. **データの可視化**：高次元データを2次元または3次元に削減して可視化しやすくする。
2. **次元削減**：モデルの計算効率を向上させるためにデータの次元を減らす。
3. **ノイズ除去**：データのノイズを除去して、より重要な信号を抽出する。

## 主成分分析の具体的な使用例

### 1. データの可視化

高次元データを2次元または3次元にプロットすることで、クラスタリングの傾向やデータのパターンを視覚的に確認できます。たとえば、手書き数字の画像データセット（MNIST）の各画像（28×28ピクセル＝784次元）をPCAで2次元に削減し、クラスタリングの可視化に使用します。

### 2. 次元削減と機械学習

多くの特徴量を持つデータセットでは、次元削減を行うことでモデルのトレーニング時間を短縮し、過学習を防ぐことができます。例えば、顔認識システムでは、画像データの次元をPCAで削減してから分類器（SVMやk-NN）に入力します。

### 3. ノイズ除去

PCAを使ってデータの主成分だけを残し、ノイズ成分を取り除くことができます。たとえば、音声データやセンサーデータのノイズ除去に使われます。

### 具体的な例：手書き数字認識

手書き数字認識（MNISTデータセット）を例にとって、PCAの使用方法を示します：

#### ステップ1：データの前処理
MNISTデータセットには、0から9までの手書き数字の28×28ピクセルのグレースケール画像が含まれています。各画像は784（28×28）次元のベクトルとして表現されます。

#### ステップ2：データのセンタリング
データ行列 $X$ をセンタリングします。各特徴量の平均を引きます：

$$ X_{\text{centered}} = X - \bar{X} $$

#### ステップ3：共分散行列の計算
センタリングしたデータの共分散行列を計算します：

$$ \Sigma = \frac{1}{n-1} X_{\text{centered}}^T X_{\text{centered}} $$

#### ステップ4：固有値分解
共分散行列 $\Sigma$ の固有値分解を行い、固有ベクトル（主成分）と固有値を求めます：

$$ \Sigma = P \Lambda P^T $$

ここで、$P$ は固有ベクトルの行列、$\Lambda$ は固有値の対角行列です。

#### ステップ5：次元削減
上位 $k$ 個の主成分を選び、データを新しい基底に変換します：

$$ X_{\text{new}} = X_{\text{centered}} P_k $$

ここで、$P_k$ は上位 $k$ 個の固有ベクトルから成る行列です。

#### ステップ6：データの可視化
次元削減後のデータ $X_{\text{new}}$ を2次元または3次元でプロットし、クラスタリングやパターンを視覚的に確認します。

### 具体例の実装コード（Python）

以下は、Pythonのライブラリを用いた具体的な実装例です：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_digits

# データの読み込み
digits = load_digits()
X = digits.data

# PCAによる次元削減
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# データの可視化
plt.figure(figsize=(8, 6))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=digits.target, cmap='viridis', edgecolor='k', s=40)
plt.legend(handles=scatter.legend_elements()[0], labels=list(range(10)), title="Digits")
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of MNIST Digits')
plt.show()
```

このコードは、MNISTデータセットをPCAで2次元に削減し、手書き数字のクラスタリングを可視化する例です。結果として、異なる数字が異なる領域にクラスタリングされる様子が視覚化されます。


```python
o
```


```python

```


```python
o
```


```python

```


```python

```


```python

```


```python
# 主成分分析を行うPythonのコード
# ライブラリは使わないで、numpyのみで実装する


import numpy as np

# データの生成
np.random.seed(1)
X = np.random.randn(5, 3)

# データの標準化
X -= X.mean(axis=0)
X /= X.std(axis=0)

# 共分散行列の計算
n_samples = X.shape[0]
cov = np.dot(X.T, X) / n_samples

# 固有値と固有ベクトルの計算
eigenvalues, eigenvectors = np.linalg.eig(cov)

# 主成分の計算
components = np.dot(X, eigenvectors)

# 結果の表示
print("X:")
print(X)
print("共分散行列:")
print(cov)
print("固有値:")
print(eigenvalues)
print("固有ベクトル:")
print(eigenvectors)
print("主成分:")
print(components)

# 結果の確認
# 主成分分析の結果をscikit-learnで確認する
from sklearn.decomposition import PCA

pca = PCA(n_components=3)
pca.fit(X)
print("scikit-learnの結果:")
print("主成分:")
print(pca.transform(X))
print("固有ベクトル:")
print(pca.components_)
print("固有値:")
print(pca.explained_variance_)
print("寄与率:")
print(pca.explained_variance_ratio_)
print("累積寄与率:")
print(np.cumsum(pca.explained_variance_ratio_))
```

    X:
    [[ 1.13040771 -0.81948067  0.11972492]
     [-1.25266968  0.84821566 -1.21361166]
     [ 1.2368398  -0.98820806  0.75671503]
     [-0.52502048  1.52188148 -1.03211241]
     [-0.58955736 -0.56240842  1.36928412]]
    共分散行列:
    [[ 1.         -0.73571666  0.46522728]
     [-0.73571666  1.         -0.84323165]
     [ 0.46522728 -0.84323165  1.        ]]
    固有値:
    [2.37432477 0.541264   0.08441124]
    固有ベクトル:
    [[-0.52969357  0.77315978  0.34878171]
     [ 0.63119463  0.08462833  0.77099376]
     [-0.56658456 -0.62853958  0.53284138]]
    主成分:
    [[-1.18385579  0.72938265 -0.17375456]
     [ 1.88653387 -0.13392777 -0.42960179]
     [-1.70764076  0.39701904  0.07269393]
     [ 1.82348234  0.37159307  0.44029139]
     [-0.81851966 -1.36406699  0.09037104]]
    scikit-learnの結果:
    主成分:
    [[-1.18385579 -0.72938265 -0.17375456]
     [ 1.88653387  0.13392777 -0.42960179]
     [-1.70764076 -0.39701904  0.07269393]
     [ 1.82348234 -0.37159307  0.44029139]
     [-0.81851966  1.36406699  0.09037104]]
    固有ベクトル:
    [[-0.52969357  0.63119463 -0.56658456]
     [-0.77315978 -0.08462833  0.62853958]
     [ 0.34878171  0.77099376  0.53284138]]
    固有値:
    [2.96790596 0.67658    0.10551405]
    寄与率:
    [0.79144159 0.18042133 0.02813708]
    累積寄与率:
    [0.79144159 0.97186292 1.        ]



```python

```


```python

```

## 結論


```python

```
