{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特異値分解と主成分分析\n",
    "\n",
    "特異値分解（SVD）と主成分分析（PCA）はデータ解析で重要な手法である。\n",
    "\n",
    "## 特異値分解（SVD）\n",
    "\n",
    "特異値分解 (Singular Value Decomposition, SVD) は行列 $A$ を以下のように分解する手法である：\n",
    "\n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "ここで $U$ は直交行列、$\\Sigma$ は特異値の対角行列、$V$ は直交行列である。SVD は行列のランクを下げるために利用され、推薦システムでは評価行列の低ランク近似により評価予測を行う：\n",
    "\n",
    "$$\n",
    "R_k = U_k \\Sigma_k V_k^T\n",
    "$$\n",
    "\n",
    "## 主成分分析（PCA）\n",
    "\n",
    "主成分分析 (Principal Component Analysis, PCA) はデータの次元削減手法である。共分散行列 $C$ の固有値分解を行い、固有ベクトルを利用して次元削減を行う。PCA も SVD を用いて実装可能である。\n",
    "\n",
    "## 推薦システムにおける応用\n",
    "\n",
    "推薦システムでは、PCA による特徴抽出や SVD による評価行列の低ランク近似により、ユーザーの好みを予測し、適切なアイテムを推薦する。\n",
    "\n",
    "\n",
    "### github\n",
    "- jupyter notebook形式のファイルは[こちら](https://github.com/hiroshi0530/wa-src/blob/master/rec/linalg/01/01_nb.ipynb)\n",
    "\n",
    "### google colaboratory\n",
    "- google colaboratory で実行する場合は[こちら](https://colab.research.google.com/github/hiroshi0530/wa-src/blob/master/rec/linalg/01/01_nb.ipynb)\n",
    "\n",
    "\n",
    "### 実行環境\n",
    "OSはmacOSです。LinuxやUnixのコマンドとはオプションが異なりますので注意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProductName:\t\tmacOS\n",
      "ProductVersion:\t\t13.5.1\n",
      "BuildVersion:\t\t22G90\n"
     ]
    }
   ],
   "source": [
    "!sw_vers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.17\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandasのテーブルを見やすいようにHTMLのテーブルにCSSの設定を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "style = \"\"\"\n",
    "<style>\n",
    "    .dataframe thead tr:only-child th {\n",
    "        text-align: right;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: left;\n",
    "        padding: 5px;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "        padding: 5px;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr:hover {\n",
    "        background-color: #ffff99;\n",
    "    }\n",
    "\n",
    "    .dataframe {\n",
    "        background-color: white;\n",
    "        color: black;\n",
    "        font-size: 16px;\n",
    "    }\n",
    "\n",
    "</style>\n",
    "\"\"\"\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本的なライブラリをインポートし watermark を利用してそのバージョンを確認しておきます。\n",
    "ついでに乱数のseedの設定をします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.9.17\n",
      "IPython version      : 8.17.2\n",
      "\n",
      "scipy     : 1.11.2\n",
      "numpy     : 1.25.2\n",
      "matplotlib: 3.8.1\n",
      "\n",
      "Watermark: 2.4.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 123\n",
    "random_state = 123\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "from watermark import watermark\n",
    "\n",
    "print(watermark(python=True, watermark=True, iversions=True, globals_=globals()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下、特異値分解と主成分分析について、Pythonの実装例を示しつつ、具体的に説明する。\n",
    "\n",
    "## 特異値分解（SVD）\n",
    "\n",
    "特異値分解 (Singular Value Decomposition, SVD) は、行列 $A$ を以下のように分解する手法である：\n",
    "\n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "ここで、\n",
    "- $A$ は $m \\times n$ 行列\n",
    "- $U$ は $m \\times m$ の直交行列\n",
    "- $\\Sigma$ は $m \\times n$ の対角行列（特異値が対角成分）\n",
    "- $V$ は $n \\times n$ の直交行列\n",
    "\n",
    "### 計算例\n",
    "\n",
    "例えば、$3 \\times 2$ 行列 $A$ を考える：\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "SVD により、$A$ を以下のように分解できる：\n",
    "\n",
    "$$\n",
    "U = \\begin{pmatrix}\n",
    "-0.2298 & 0.8835 & 0.4082 \\\\\n",
    "-0.5247 & 0.2408 & -0.8165 \\\\\n",
    "-0.8196 & -0.4018 & 0.4082\n",
    "\\end{pmatrix}, \\quad\n",
    "\\Sigma = \\begin{pmatrix}\n",
    "9.5255 & 0 \\\\\n",
    "0 & 0.5143 \\\\\n",
    "0 & 0\n",
    "\\end{pmatrix}, \\quad\n",
    "V = \\begin{pmatrix}\n",
    "-0.6196 & -0.7849 \\\\\n",
    "-0.7849 & 0.6196\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### 推薦システムへの応用\n",
    "\n",
    "推薦システムでは、評価行列 $R$ の低ランク近似に SVD を用いる。例えば、$R$ を次のように分解し、$k$ 個の特異値を使って近似する：\n",
    "\n",
    "$$\n",
    "R_k = U_k \\Sigma_k V_k^T\n",
    "$$\n",
    "\n",
    "この近似により、データのノイズを除去しつつ、評価予測の精度を向上させることができる。\n",
    "\n",
    "### メリットとデメリット\n",
    "\n",
    "SVD のメリットは、データの低ランク近似により、データの圧縮やノイズの除去が可能である点である。また、行列のランクを下げることで、計算量を減らすことができる。一方で、SVD は計算コストが高く、大規模データには適用が難しい場合がある。また、特異値の選択方法によっては、情報の損失が発生する可能性がある。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 主成分分析（PCA）\n",
    "\n",
    "主成分分析 (Principal Component Analysis, PCA) は、データの次元削減手法である。PCA は以下の手順で行う：\n",
    "\n",
    "1. データ行列 $X$ の中心化（平均を引く）\n",
    "2. 共分散行列 $C = \\frac{1}{n} X^T X$ の計算\n",
    "3. 共分散行列 $C$ の固有値分解を行い、固有値と固有ベクトルを求める\n",
    "4. 固有値の大きい順に固有ベクトルを選び次元削減\n",
    "\n",
    "### 計算例\n",
    "\n",
    "例えば、データ行列 $X$ を以下のように定義する：\n",
    "\n",
    "$$\n",
    "X = \\begin{pmatrix}\n",
    "2.5 & 2.4 \\\\\n",
    "0.5 & 0.7 \\\\\n",
    "2.2 & 2.9 \\\\\n",
    "1.9 & 2.2 \\\\\n",
    "3.1 & 3.0 \\\\\n",
    "2.3 & 2.7 \\\\\n",
    "2.0 & 1.6 \\\\\n",
    "1.0 & 1.1 \\\\\n",
    "1.5 & 1.6 \\\\\n",
    "1.1 & 0.9\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "共分散行列 $C$ を計算すると：\n",
    "\n",
    "$$\n",
    "C = \\begin{pmatrix}\n",
    "0.6166 & 0.6154 \\\\\n",
    "0.6154 & 0.7166\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "この共分散行列の固有値と固有ベクトルを求めると、主成分を得ることができる。\n",
    "\n",
    "### 推薦システムへの応用\n",
    "\n",
    "PCA はユーザーやアイテムの特徴を抽出し、次元削減を行うために利用される。例えば、映画推薦システムでは、ユーザーの評価データを主成分分析により特徴ベクトルに変換し、ユーザーの好みに合った映画を推薦することが可能である。\n",
    "\n",
    "### メリットとデメリット\n",
    "\n",
    "PCA のメリットは、次元削減によりデータの可視化や計算効率の向上が図れる点である。また、データの分散が最大となる方向を見つけ出すため、データの重要な情報を保持することができる。一方で、PCA は線形性の仮定に依存しており、非線形な関係を持つデータには適用が難しい。また、固有値の選択方法によっては、情報の損失が発生する可能性がある。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[[ 1.13040771 -0.81948067  0.11972492]\n",
      " [-1.25266968  0.84821566 -1.21361166]\n",
      " [ 1.2368398  -0.98820806  0.75671503]\n",
      " [-0.52502048  1.52188148 -1.03211241]\n",
      " [-0.58955736 -0.56240842  1.36928412]]\n",
      "共分散行列:\n",
      "[[ 1.         -0.73571666  0.46522728]\n",
      " [-0.73571666  1.         -0.84323165]\n",
      " [ 0.46522728 -0.84323165  1.        ]]\n",
      "固有値:\n",
      "[2.37432477 0.541264   0.08441124]\n",
      "固有ベクトル:\n",
      "[[-0.52969357  0.77315978  0.34878171]\n",
      " [ 0.63119463  0.08462833  0.77099376]\n",
      " [-0.56658456 -0.62853958  0.53284138]]\n",
      "主成分:\n",
      "[[-1.18385579  0.72938265 -0.17375456]\n",
      " [ 1.88653387 -0.13392777 -0.42960179]\n",
      " [-1.70764076  0.39701904  0.07269393]\n",
      " [ 1.82348234  0.37159307  0.44029139]\n",
      " [-0.81851966 -1.36406699  0.09037104]]\n",
      "scikit-learnの結果:\n",
      "主成分:\n",
      "[[-1.18385579 -0.72938265 -0.17375456]\n",
      " [ 1.88653387  0.13392777 -0.42960179]\n",
      " [-1.70764076 -0.39701904  0.07269393]\n",
      " [ 1.82348234 -0.37159307  0.44029139]\n",
      " [-0.81851966  1.36406699  0.09037104]]\n",
      "固有ベクトル:\n",
      "[[-0.52969357  0.63119463 -0.56658456]\n",
      " [-0.77315978 -0.08462833  0.62853958]\n",
      " [ 0.34878171  0.77099376  0.53284138]]\n",
      "固有値:\n",
      "[2.96790596 0.67658    0.10551405]\n",
      "寄与率:\n",
      "[0.79144159 0.18042133 0.02813708]\n",
      "累積寄与率:\n",
      "[0.79144159 0.97186292 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# 主成分分析を行うPythonのコード\n",
    "# ライブラリは使わないで、numpyのみで実装する\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# データの生成\n",
    "np.random.seed(1)\n",
    "X = np.random.randn(5, 3)\n",
    "\n",
    "# データの標準化\n",
    "X -= X.mean(axis=0)\n",
    "X /= X.std(axis=0)\n",
    "\n",
    "# 共分散行列の計算\n",
    "n_samples = X.shape[0]\n",
    "cov = np.dot(X.T, X) / n_samples\n",
    "\n",
    "# 固有値と固有ベクトルの計算\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "\n",
    "# 主成分の計算\n",
    "components = np.dot(X, eigenvectors)\n",
    "\n",
    "# 結果の表示\n",
    "print(\"X:\")\n",
    "print(X)\n",
    "print(\"共分散行列:\")\n",
    "print(cov)\n",
    "print(\"固有値:\")\n",
    "print(eigenvalues)\n",
    "print(\"固有ベクトル:\")\n",
    "print(eigenvectors)\n",
    "print(\"主成分:\")\n",
    "print(components)\n",
    "\n",
    "# 結果の確認\n",
    "# 主成分分析の結果をscikit-learnで確認する\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X)\n",
    "print(\"scikit-learnの結果:\")\n",
    "print(\"主成分:\")\n",
    "print(pca.transform(X))\n",
    "print(\"固有ベクトル:\")\n",
    "print(pca.components_)\n",
    "print(\"固有値:\")\n",
    "print(pca.explained_variance_)\n",
    "print(\"寄与率:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(\"累積寄与率:\")\n",
    "print(np.cumsum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本記事では、特異値分解と主成分分析についてPythonの実装を交えつつ解説した。\n",
    "\n",
    "特異値分解と主成分分析は、データの次元削減や特徴抽出に有効な手法であり、推薦システムにおいても重要な役割を果たす。SVD による評価行列の低ランク近似や PCA による特徴抽出により、ユーザーの好みを予測し、適切なアイテムを推薦することができる。これらの手法を適用する際には、メリットとデメリットを理解し、適切な方法を選択することが重要である。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
